{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Artificial Intelligence Engineer Nanodegree - Probabilistic Models\n",
    "## Project: Sign Language Recognition System\n",
    "- [Introduction](#intro)\n",
    "- [Part 1 Feature Selection](#part1_tutorial)\n",
    "    - [Tutorial](#part1_tutorial)\n",
    "    - [Features Submission](#part1_submission)\n",
    "    - [Features Unittest](#part1_test)\n",
    "- [Part 2 Train the models](#part2_tutorial)\n",
    "    - [Tutorial](#part2_tutorial)\n",
    "    - [Model Selection Score Submission](#part2_submission)\n",
    "    - [Model Score Unittest](#part2_test)\n",
    "- [Part 3 Build a Recognizer](#part3_tutorial)\n",
    "    - [Tutorial](#part3_tutorial)\n",
    "    - [Recognizer Submission](#part3_submission)\n",
    "    - [Recognizer Unittest](#part3_test)\n",
    "- [Part 4 (OPTIONAL) Improve the WER with Language Models](#part4_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "* **Goal** - build a word recognizer for American Sign Language (ASL) video sequences, demonstrating the power of probabalistic models.  \n",
    "* **Approach** - [Hidden Markov models (HMM's)](https://en.wikipedia.org/wiki/Hidden_Markov_model) to analyze a series of measurements taken from ASL videos collected for research (see the [RWTH-BOSTON-104 Database](http://www-i6.informatik.rwth-aachen.de/~dreuw/database-rwth-boston-104.php)).  In the video shown, the right-hand x and y locations are plotted as the speaker produces sign language (signs) the sentence.\n",
    "[![ASLR demo](http://www-i6.informatik.rwth-aachen.de/~dreuw/images/demosample.png)](https://drive.google.com/open?id=0B_5qGuFe-wbhUXRuVnNZVnMtam8)\n",
    "\n",
    "* **Inputs Pre-defined**: raw data, train, and test sets\n",
    "* **Part 1**: Derive variety of feature sets. \n",
    "* **Part 2**: Implement 3x different model selection criterion to determine the optimal number of hidden states for each word model. \n",
    "* **Part 3**: Implement recognizer. Compare effects of the different combinations of feature sets and model selection criteria.  \n",
    "* **Outputs**: At the end of each Part, complete the submission cells with implementations, answer all questions, pass all unit tests, then submit completed notebook for review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part1_tutorial'></a>\n",
    "## PART 1: Data\n",
    "\n",
    "### Features Tutorial\n",
    "##### Load the initial database\n",
    "* **Inputs** - In **asl_data.py** (module) a **Data Handler** designed for this database is in the `AslDb` class.  It creates the initial [pandas](http://pandas.pydata.org/pandas-docs/stable/) dataframe from data included in the `data` directory as well as dictionaries suitable for extracting data in a format friendly to the [hmmlearn](https://hmmlearn.readthedocs.io/en/latest/) library, which are used to create models in Part 2.\n",
    "\n",
    "First set up initial database. Select an example set of features for the training set.  At the end of Part 1, you will create additional feature sets for experimentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "###_DEBUG_IDE\n",
    "\n",
    "# # Auto-reload all changed modules every time before executing a new line\n",
    "# %load_ext autoreload \n",
    "# %autoreload 2\n",
    "\n",
    "def test():\n",
    "    print(\"Called .ipynb file from .py file using notebook_importing.py\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from asl_data import AslDb\n",
    "\n",
    "# Initializes the database\n",
    "asl = AslDb()\n",
    "\n",
    "# Display first five rows (pandas data frame) of the ASL database, \n",
    "# indexed by video and frame\n",
    "\n",
    "# print(asl.df.__dict__)\n",
    "\"\"\"\n",
    "{  'is_copy': None, '_data': \n",
    "    BlockManager Items: \n",
    "        Index(['left-x', 'left-y', 'right-x', 'right-y', 'nose-x', 'nose-y', 'speaker'], \n",
    "          dtype='object')\n",
    "    Axis 1: MultiIndex(levels=[[1, 2, 3, .., 200, 201], \n",
    "                               [0, 1, 2, ..., 160, 161]],\n",
    "                       labels=[[97, 97, ..., 124, 124], \n",
    "                               [0, 1, 2,  56]], \n",
    "                       names=['video', 'frame'])\n",
    "    IntBlock: slice(0, 6, 1), 6 x 15746, dtype: int64\n",
    "    ObjectBlock: slice(6, 7, 1), 1 x 15746, dtype: object, '_item_cache': {}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "asl.df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Display data available for an individual frame\n",
    "# Reference: `asl_utils.py`\n",
    "\n",
    "asl.df.ix[98,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The frame represented by video 98, frame 1 is shown here:\n",
    "![Video 98](http://www-i6.informatik.rwth-aachen.de/~dreuw/database/rwth-boston-104/overview/images/orig/098-start.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Frame represented by video 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Display data available for an individual frame\n",
    "# Reference: `asl_utils.py`\n",
    "\n",
    "asl.df.ix[150,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The frame represented by video 150, frame 1 is shown here:\n",
    "![Video 150](http://www-i6.informatik.rwth-aachen.de/~dreuw/database/rwth-boston-104/overview/images/orig/150-start.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Feature selection for training the model\n",
    "* About: Feature Selection when training a model\n",
    "* Objectives: \n",
    "    * Choose most relevant variables.\n",
    "    * Keeping model as simple as possible to reduce training time.  \n",
    "* Inputs: \n",
    "    * Raw features (already provided)\n",
    "    * Custom derivation of our own. Add columns to pandas dataframe `asl.df` for selection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Calculate the feature named 'grnd-ry' (\"ground\" right y value), which is \n",
    "# the difference b/w right-hand y value and nose y value. \n",
    "# Add it to the `asl.df` frames dictionary\n",
    "# (append result as column label/values to the right of Table) data frame \n",
    "\n",
    "asl.df['grnd-ry'] = asl.df['right-y'] - asl.df['nose-y']\n",
    "asl.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Run\n",
    "\n",
    "* Click the code snippet cell below and select \"Run All\"\n",
    "\n",
    "### Task DONE\n",
    "\n",
    "* Add df columns for 'grnd-rx', 'grnd-ly', 'grnd-lx' representing differences between hand and nose locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from asl_utils import test_features_tryit\n",
    "\n",
    "# DONE \n",
    "# Add df columns for 'grnd-rx', 'grnd-ly', 'grnd-lx' representing \n",
    "# differences between hand and nose locations\n",
    "asl.df['grnd-ry'] = asl.df['right-y'] - asl.df['nose-y']\n",
    "asl.df['grnd-rx'] = asl.df['right-x'] - asl.df['nose-x']\n",
    "asl.df['grnd-ly'] = asl.df['left-y'] - asl.df['nose-y']\n",
    "asl.df['grnd-lx'] = asl.df['left-x'] - asl.df['nose-x']\n",
    "\n",
    "asl.df.head()\n",
    "\n",
    "# Test the code\n",
    "# Expected: [9, 113, -12, 119]\n",
    "# test_features_tryit(asl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Collect the features into a list\n",
    "features_ground = ['grnd-rx','grnd-ry','grnd-lx','grnd-ly']\n",
    "\n",
    "# Show a single set of features for a given (video, frame) tuple\n",
    "[asl.df.ix[98,1][v] for v in features_ground]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Build the training set\n",
    "\n",
    "* Collect the feature for all the Words in the training set by passing the defined feature list `features_ground` to the `AslDb.build_training` method of `asl_data.py`. Each WordData object (see `WordsData` class in `asl_data.py` in the training set has multiple examples from various videos suitable for the hmmlearn library. \n",
    "* WordData object is dictionary of lists of feature list sequence lists for each word `{'FRANK': [[[87, 225], [87, 225], ...], [[88, 219], [88, 219], ...]]]}`.\n",
    "* Shown below are unique words that have been loaded into the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training = asl.build_training(features_ground)\n",
    "\n",
    "print(\"Training words - num_items: {}\".format(training.num_items))\n",
    "print(\"Training words - words: {}\".format(training.words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Training data object `training` is an instance of class `WordsData` built by calling the `build_training` method of class `AslDb` defined in the `asl_data.py` module. Access data using:\n",
    "    * `WordsData` instance properties:\n",
    "        * `words` - get list of keys\n",
    "    * `WordsData` instance methods:\n",
    "        * `get_all_sequences` - getter for entire db of words as series of sequences of feature lists for each frame\n",
    "        * `get_all_Xlengths` - getter for entire db of words as (X, lengths) tuple for use training multiple sequences with the `hmmlearn` library. where X is a numpy array of feature lists and lengths is list of lengths of sequences within X. i.e. `{'FRANK': (array([[ 87, 225],[ 87, 225], ... [ 87, 225,  62, 127], [ 87, 225,  65, 128]]), [14, 18]), }`\n",
    "        * `get_word_sequences` - getter for single word series of sequences of feature lists for each frame\n",
    "        * `get_word_Xlengths` - getter for single word (X, lengths) tuple for use with hmmlearn library, where X is a numpy array of feature lists and lengths is a list of lengths of sequences within X\n",
    "\n",
    "* The following example outputs two nested lists:\n",
    "    * First list is a concatenation of all the sequences(the X portion)\n",
    "    * Second is a list of the sequence lengths(the Lengths portion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training.get_word_Xlengths('CHOCOLATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### More feature sets\n",
    "* `asl.df` is still a Simple \"feature set\", but is enough to started modeling.\n",
    "* Improve results fom `asl.df` by manipulating the raw values. Below we setup other options for later experimentation. \n",
    "    * Option 1: **Grouped Statistics** to normalize each speaker's range of motion using [Pandas stats](http://pandas.pydata.org/pandas-docs/stable/api.html#api-dataframe-stats) functions and [pandas groupby](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html). See below example for finding the Means of all speaker subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_means = asl.df.groupby('speaker', 0, None, True, True, True, False).mean()\n",
    "df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To select a mean that matches by speaker, use the pandas [map](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# asl.df['left-x-mean']= asl.df['speaker'].map(df_means['left-x'], na_action=None)\n",
    "asl.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Run\n",
    "\n",
    "* Click the code snippet cell below and select \"Run All\"\n",
    "\n",
    "### Task DONE\n",
    "\n",
    "* Create a dataframe named `df_std` with standard deviations grouped by speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from asl_utils import test_std_tryit\n",
    "# DONE Create a dataframe named `df_std` with standard deviations grouped by speaker\n",
    "\n",
    "df_std = asl.df.groupby('speaker', 0, None, True, True, True, False).std()\n",
    "\n",
    "asl.df['left-x-mean'] = asl.df['speaker'].map(df_means['left-x'], na_action=None)\n",
    "asl.df['left-x-std'] = asl.df['speaker'].map(df_std['left-x'], na_action=None)\n",
    "asl.df['left-y-mean'] = asl.df['speaker'].map(df_means['left-y'], na_action=None)\n",
    "asl.df['left-y-std'] = asl.df['speaker'].map(df_std['left-y'], na_action=None)\n",
    "\n",
    "asl.df['right-x-mean'] = asl.df['speaker'].map(df_means['right-x'], na_action=None)\n",
    "asl.df['right-x-std'] = asl.df['speaker'].map(df_std['right-x'], na_action=None)\n",
    "asl.df['right-y-mean'] = asl.df['speaker'].map(df_means['right-y'], na_action=None)\n",
    "asl.df['right-y-std'] = asl.df['speaker'].map(df_std['right-y'], na_action=None)\n",
    "\n",
    "asl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test the code\n",
    "test_std_tryit(df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part1_submission'></a>\n",
    "### Features Implementation Submission\n",
    "Implement four feature sets and answer the question that follows.\n",
    "- normalized Cartesian coordinates\n",
    "    - use *mean* and *standard deviation* statistics and the [standard score](https://en.wikipedia.org/wiki/Standard_score) equation to account for speakers with different heights and arm length\n",
    "    \n",
    "```\n",
    "Standard Score (of a raw score x)\n",
    "\n",
    "z = x - m / std\n",
    "\n",
    "where m is the mean\n",
    "where std is the standard deviation\n",
    "```\n",
    "\n",
    "- polar coordinates\n",
    "    - calculate polar coordinates with [Cartesian to polar equations](https://en.wikipedia.org/wiki/Polar_coordinate_system#Converting_between_polar_and_Cartesian_coordinates)\n",
    "    - use the [np.arctan2](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.arctan2.html) function and *swap the x and y axes* to move the $0$ to $2\\pi$ discontinuity to 12 o'clock instead of 3 o'clock;  in other words, the normal break in radians value from $0$ to $2\\pi$ occurs directly to the left of the speaker's nose, which may be in the signing area and interfere with results.  By swapping the x and y axes, that discontinuity move to directly above the speaker's head, an area not generally used in signing.\n",
    "\n",
    "- delta difference\n",
    "    - as described in Thad's lecture, use the difference in values between one frame and the next frames as features. Videos number 12 and 13 briefly mention using delta-y as a feature. The idea of using delta y as a feature is that the direction (i.e. positive or negative y) and the speed of movement between successive frames could be more useful than where the hand is exactly. delta values for the first frame of each video should be zero.\n",
    "    - pandas [diff method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.diff.html) and [fillna method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) will be helpful for this one\n",
    "\n",
    "- custom features\n",
    "    - These are your own design; combine techniques used above or come up with something else entirely. We look forward to seeing what you come up with! \n",
    "    Some ideas to get you started:\n",
    "        - normalize using a [feature scaling equation](https://en.wikipedia.org/wiki/Feature_scaling)\n",
    "        - normalize the polar coordinates\n",
    "        - adding additional deltas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DONE add features for normalized by speaker values of left, right, x, y\n",
    "# Name these 'norm-rx', 'norm-ry', 'norm-lx', and 'norm-ly'\n",
    "# using Z-score scaling (X-Xmean)/Xstd \n",
    "\n",
    "# Standard Score equation to normalise Cartesian coordinates x and y\n",
    "def standard_score(x, m, std):\n",
    "    return (x - m) / std\n",
    "\n",
    "features_r_l = ['right-x', 'right-y', 'left-x', 'left-y']\n",
    "\n",
    "features_norm = ['norm-rx', 'norm-ry', 'norm-lx', 'norm-ly']\n",
    "\n",
    "for f_index, f_val in enumerate(features_norm):\n",
    "    f_mean = asl.df['speaker'].map(df_means[features_r_l[f_index]], na_action=None)\n",
    "    f_std  = asl.df['speaker'].map(df_std[features_r_l[f_index]], na_action=None)\n",
    "    asl.df[f_val] = standard_score(asl.df[features_r_l[f_index]], f_mean, f_std)\n",
    "\n",
    "asl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DONE add features for polar coordinate values where the nose is the origin\n",
    "# Name these 'polar-rr', 'polar-rtheta', 'polar-lr', and 'polar-ltheta'\n",
    "# Note that 'polar-rr' and 'polar-rtheta' refer to the radius and angle\n",
    "\n",
    "# Convert Cartesian coordinates x and y to polar coords with r ≥ 0 and ϕ (phi) in the interval (−pi, pi] by:\n",
    "#\n",
    "# radius = sqrt(x^2 + y^2)\n",
    "\n",
    "# Use the np.arctan2 function for tan-1 ( y / x ) and swap the x and y axes to move the 0 to 2pi discontinuity to 12 o'clock \n",
    "# instead of 3 o'clock (i.e. the normal break in radians value from 0 to 2pi occurs directly to the left \n",
    "# of the speaker's nose, which may be in the signing area and interfere with results, but by swapping \n",
    "# the x and y axes, that discontinuity moves to directly above the speaker's head area that's not generally\n",
    "# used in signing.\n",
    "#\n",
    "# angle = np.arctan2(y, x)   \n",
    "\n",
    "# Convert Cartesian coordinates x and y to radius (Euclidean Norm) in Polar Coordinates \n",
    "def radius(x, y):\n",
    "    return np.sqrt(x**2 + y**2)\n",
    "\n",
    "# Convert Cartesian coordinates x and y to angle (theta) in Polar Coordinates \n",
    "def angle(x, y):\n",
    "    # Swap the x and y axes in the calculation\n",
    "    # (i.e. instead of `np.arctan2(y, x)`)\n",
    "    return np.arctan2(x, y)\n",
    "\n",
    "# Differences between hand and nose locations (see previous task where calculated)\n",
    "features_ground = ['grnd-rx', 'grnd-ry', 'grnd-lx', 'grnd-ly']\n",
    "\n",
    "features_polar = ['polar-rr', 'polar-rtheta', 'polar-lr', 'polar-ltheta']\n",
    "\n",
    "for f_index, f_val in enumerate(features_polar):\n",
    "    if f_index % 2 == 0: # even indexes (i.e. 0, 2)\n",
    "        asl.df[f_val] = radius(asl.df[features_ground[f_index]], \n",
    "                               asl.df[features_ground[f_index + 1]])\n",
    "    else: # odd indexes\n",
    "        asl.df[f_val] = angle(asl.df[features_ground[f_index - 1]], \n",
    "                              asl.df[features_ground[f_index]])\n",
    "asl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DONE add features for left, right, x, y differences by one time step, i.e. the \"delta\" values discussed in the lecture\n",
    "# Name these 'delta-rx', 'delta-ry', 'delta-lx', and 'delta-ly'\n",
    "\n",
    "# Replace any NaN values with 0 after taking the results of the difference\n",
    "def frame_difference(f):\n",
    "    return f.diff().fillna(0, None, 0, False, None, None)\n",
    "\n",
    "features_r_l = ['right-x', 'right-y', 'left-x', 'left-y']\n",
    "\n",
    "# Features representing the difference in values between one frame and the next frames \n",
    "features_delta = ['delta-rx', 'delta-ry', 'delta-lx', 'delta-ly']\n",
    "\n",
    "for f_index, f_val in enumerate(features_delta):\n",
    "    asl.df[f_val] = frame_difference(asl.df[features_r_l[f_index]])\n",
    "\n",
    "asl.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DONE add features of your own design, which may be a combination of the above or something else\n",
    "# Name these whatever you would like, such as:\n",
    "#   - normalize using a Feature Scaling equation\n",
    "\n",
    "# Rescaling the range of features to scale to the range in [0, 1] or [-1, 1]\n",
    "# using the Feature Scaling equation to normalise:\n",
    "#\n",
    "#   normalised = orig - min(orig) / max(orig) - min(orig)\n",
    "#\n",
    "def rescale(orig, min, max):\n",
    "    return (orig - min) / (max - min)\n",
    "\n",
    "df_max = asl.df.groupby('speaker').max()\n",
    "df_min = asl.df.groupby('speaker').min()\n",
    "\n",
    "# Differences between hand and nose locations (see previous task where calculated)\n",
    "features_ground = ['grnd-rx', 'grnd-ry', 'grnd-lx', 'grnd-ly']\n",
    "\n",
    "# Features representing the Max and Min \n",
    "features_max = ['right-x-max', 'right-y-max', 'left-x-max', 'left-y-max']\n",
    "features_min = ['right-x-min', 'right-y-min', 'left-x-min', 'left-y-min']\n",
    "\n",
    "features_rescaled = ['right-x-rescaled', 'right-y-rescaled', 'left-x-rescaled', 'left-y-rescaled']\n",
    "\n",
    "for f_index, f_val in enumerate(features_ground):\n",
    "\n",
    "    # Map the Max and Min onto the data\n",
    "    asl.df[features_max[f_index]] = asl.df['speaker'].map(df_max[f_val], na_action=None)\n",
    "    asl.df[features_min[f_index]] = asl.df['speaker'].map(df_min[f_val], na_action=None)\n",
    "    \n",
    "    # Normalise by rescaling using the Feature Scaling equation\n",
    "    asl.df[features_rescaled[f_index]] = rescale(asl.df[f_val],\n",
    "                                               asl.df[features_min[f_index]],\n",
    "                                               asl.df[features_max[f_index]])\n",
    "\n",
    "# DONE define list named 'features_custom' for building the training set\n",
    "features_custom = ['right-x-rescaling', 'right-y-rescaling',  'left-x-rescaling', 'left-y-rescaling']\n",
    "\n",
    "asl.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 1:**  What custom features did you choose for the features_custom set and why?\n",
    "\n",
    "**Answer 1:** The custom feature chosen for the features_custom was to rescale the features into a lower range between 0 and 1 for the Cartesian coordinates x and y use the Feature Scaling equation to achieve the normalisation. \n",
    "\n",
    "The benefits of rescaling are that interpretation and analysis of plotted data becomes easier. In stochastic environments such as the one we are using, rescaling is known to improve the convergence speed of gradient-descent. We are using similar gradient-based optimisation methods such as the iterative Baum-Welch algorithm (aka Expectation-Maximization (EM) algorithm) to solve the third fundamental problem of HMMs by estimating the model parameters given just the observed data. Rescaling is therefore important since HMMs already have a large vocabulary of signs and use a lot of memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part1_test'></a>\n",
    "### Features Unit Testing\n",
    "Run the following unit tests as a sanity check on the defined \"ground\", \"norm\", \"polar\", and 'delta\"\n",
    "feature sets.  The test simply looks for some valid values but is not exhaustive.  However, the project should not be submitted if these tests don't pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Verify values of features before tests\n",
    "\n",
    "# Collect the features into a list\n",
    "features_ground = ['grnd-rx','grnd-ry','grnd-lx','grnd-ly']\n",
    "features_norm = ['norm-rx', 'norm-ry', 'norm-lx', 'norm-ly']\n",
    "features_polar = ['polar-rr', 'polar-rtheta', 'polar-lr', 'polar-ltheta']\n",
    "features_delta = ['delta-rx', 'delta-ry', 'delta-lx', 'delta-ly']\n",
    "\n",
    "# Show a single set of features for a given (video, frame) tuple\n",
    "print([asl.df.ix[98,1][v] for v in features_ground])\n",
    "print([asl.df.ix[98,1][v] for v in features_norm])\n",
    "print([asl.df.ix[98,1][v] for v in features_polar])\n",
    "print([asl.df.ix[98,1][v] for v in features_delta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "# import numpy as np\n",
    "\n",
    "class TestFeatures(unittest.TestCase):\n",
    "\n",
    "    def test_features_ground(self):\n",
    "        sample = (asl.df.ix[98, 1][features_ground]).tolist()\n",
    "        self.assertEqual(sample, [9, 113, -12, 119])\n",
    "\n",
    "    def test_features_norm(self):\n",
    "        sample = (asl.df.ix[98, 1][features_norm]).tolist()\n",
    "        np.testing.assert_almost_equal(sample, [ 1.153,  1.663, -0.891,  0.742], 3)\n",
    "\n",
    "    def test_features_polar(self):\n",
    "        sample = (asl.df.ix[98,1][features_polar]).tolist()\n",
    "        np.testing.assert_almost_equal(sample, [113.3578, 0.0794, 119.603, -0.1005], 3)\n",
    "\n",
    "    def test_features_delta(self):\n",
    "        sample = (asl.df.ix[98, 0][features_delta]).tolist()\n",
    "        self.assertEqual(sample, [0, 0, 0, 0])\n",
    "        sample = (asl.df.ix[98, 18][features_delta]).tolist()\n",
    "        self.assertTrue(sample in [[-16, -5, -2, 4], [-14, -9, 0, 0]], \"Sample value found was {}\".format(sample))\n",
    "                         \n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestFeatures())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part2_tutorial'></a>\n",
    "## PART 2: Model Selection\n",
    "### Model Selection Tutorial\n",
    "* Objective: Model Selection should tune the number of states for each word HMM prior to testing on unseen data. 3x tuning methods include: \n",
    "    * Log likelihood using Cross-Validation folds (CV)\n",
    "    * Bayesian Information Criterion (BIC)\n",
    "    * Discriminative Information Criterion (DIC) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Train a single word\n",
    "* \"train\" models for each word using the training set we built with sequence data. \n",
    "* e.g. Train a single word using Gaussian hidden Markov models (HMM) using the `fit` method during training. [Baum-Welch Expectation-Maximization](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm) (EM) algorithm is invoked iteratively to find the best estimate for the model (for the number of hidden states specified) from a group of sample sequences. \n",
    "    * Assume the correct number of hidden states is 3 (a guess). The \"best\" number of states for training is determined by some model selection technique that chooses the \"best\" parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "def train_a_word(word, num_hidden_states, features):\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    \n",
    "    # Build training set\n",
    "    training = asl.build_training(features)\n",
    "    \n",
    "    # Call getter for single word (X, lengths) tuple for use with hmmlearn library,\n",
    "    # where \"X\" is a numpy array of feature lists\n",
    "    # where \"lengths\" is a list of lengths of sequences within X\n",
    "    X, lengths = training.get_word_Xlengths(word)\n",
    "    \n",
    "    # Build instance of Hidden Markov Model (HMM) with Gaussian emissions\n",
    "    # - \"fit\" method trains the HMM using the input matrix of concatenated sequences\n",
    "    #   of observations (aka samples), and the lengths of each seqence.\n",
    "    # - \"n_components\" attribute is the number of hidden States in the HMM\n",
    "    # - \"n_iter\" is number of Baum-Welch (EM algorithm) iterations\n",
    "    #\n",
    "    # Reference: \n",
    "    # - http://hmmlearn.readthedocs.io/en/latest/tutorial.html\n",
    "    # - http://hmmlearn.readthedocs.io/en/latest/api.html\n",
    "    model = GaussianHMM(n_components=num_hidden_states, n_iter=1000).fit(X, lengths)\n",
    "    \n",
    "    # \"score\" method calculates the log likelihood score of the HMM sample(s). \n",
    "    # Since the Baum-Welch algorithm\n",
    "    # (aka Expectation-Maximization (EM) algorithm) is a gradient-based optimisation method\n",
    "    # we try to avoid it getting stuck in local optima but running the \"fit\" method with\n",
    "    # different initialisations, and then selecting the highest scored model.\n",
    "    logL = model.score(X, lengths)\n",
    "    return model, logL\n",
    "\n",
    "demoword = 'BOOK'\n",
    "model, logL = train_a_word(demoword, 3, features_ground)\n",
    "print(\"Built instance of a Hidden Markov Model (HMM) with Gaussian emissions using hmmlearn\")\n",
    "print(\"Number of states trained in model for {} is {}\".format(demoword, model.n_components))\n",
    "print(\"logL = {}\".format(logL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The HMM model has been trained and information can be pulled from the model, including means and variances for each feature and hidden state.  The [log likelihood](http://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution) for any individual sample or group of samples can also be calculated with the `score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def show_model_stats(word, model):\n",
    "    print(\"Number of states trained in model for word {} is {}\\n\".format(word, model.n_components))    \n",
    "    variance = np.array([np.diag(model.covars_[i]) for i in range(model.n_components)])    \n",
    "    for i in range(model.n_components):  # for each hidden state\n",
    "        print(\"\\t - Hidden state #{}\".format(i))\n",
    "        print(\"\\t - Mean = \", model.means_[i])\n",
    "        print(\"\\t - Variance = \", variance[i])\n",
    "        print()\n",
    "    \n",
    "show_model_stats(demoword, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Try it!\n",
    "Experiment by changing the feature set, word, and/or num_hidden_states values in the next cell to see changes in values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Replace with any word key contained in `training.words`\n",
    "# (i.e. CHOCOLATE, BOOK, etc)\n",
    "my_testword = 'CHOCOLATE'\n",
    "\n",
    "# Experiment here with different parameters\n",
    "# (i.e. Number of States: 3, 5, 7, 12)\n",
    "# (i.e. Feature Set: features_ground, features_norm, \n",
    "#  features_polar, features_delta)\n",
    "model, logL = train_a_word(my_testword, 12, features_ground)\n",
    "show_model_stats(my_testword, model)\n",
    "print(\"logL = {}\".format(logL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Visualize the hidden states\n",
    "\n",
    "* Plot the Means and Variances for each State and Feature.  \n",
    "* Experiment by varying the Number of States Trained for the HMM model and examine the variances. \n",
    "\n",
    "1) Are some models are \"better\" than others?  \n",
    "\n",
    "2) How can you tell?  \n",
    "\n",
    "3) Write what you think in the classroom online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib import (cm, pyplot as plt, mlab)\n",
    "\n",
    "def visualize(word, model):\n",
    "    \"\"\" visualize the input model for a particular word \"\"\"\n",
    "    \n",
    "    # Variance of Hidden States\n",
    "    variance = np.array([np.diag(model.covars_[i]) for i in range(model.n_components)])\n",
    " \n",
    "    figures = []\n",
    "    for parm_idx in range(len(model.means_[0])):\n",
    "        \n",
    "        print(\"parma_idx: {} means: {} variance: {}\".format(parm_idx, model.means_[:,parm_idx], variance[:,parm_idx]))\n",
    "        \n",
    "        # xmin = int(min(model.means_[:,parm_idx]) - max(variance[:,parm_idx]))\n",
    "        # xmax = int(max(model.means_[:,parm_idx]) + max(variance[:,parm_idx]))\n",
    "        \n",
    "        xmin = int((min(model.means_[:,parm_idx]) - max(variance[:,parm_idx]))*100)/100\n",
    "        xmax = int((max(model.means_[:,parm_idx]) + max(variance[:,parm_idx]))*100)/100\n",
    "        \n",
    "        fig, axs = plt.subplots(model.n_components, sharex=True, sharey=False)\n",
    "        colours = cm.rainbow(np.linspace(0, 1, model.n_components))\n",
    "        for i, (ax, colour) in enumerate(zip(axs, colours)):\n",
    "            x = np.linspace(xmin, xmax, 100)\n",
    "            mu = model.means_[i, parm_idx]\n",
    "            sigma = math.sqrt(np.diag(model.covars_[i])[parm_idx])\n",
    "            \n",
    "            print(\"parma_idx: {} i: {} mu: {} sigma: {} xmin: {} xmax: {}\".format(parm_idx, i, mu, sigma, xmin, xmax))\n",
    "            \n",
    "            ax.plot(x, mlab.normpdf(x, mu, sigma), c=colour)\n",
    "            ax.set_title(\"{} feature {} hidden state #{}\".format(word, parm_idx, i))\n",
    "            ax.grid(True)\n",
    "        figures.append(plt)\n",
    "    for p in figures:\n",
    "        p.show()\n",
    "        \n",
    "visualize(my_testword, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#####  ModelSelector class\n",
    "* Review `SelectorModel` class in `my_model_selectors.py` module.  \n",
    "* `SelectorModel` is designed as a Strategy Pattern for choosing different Model Selectors.  \n",
    "* Subclass the `SelectorModel` to implement the following model selectors classes/functions in the `my_model_selectors.py` module. Run them from this notebook:\n",
    "\n",
    "    *`SelectorCV `:  Log likelihood with CV\n",
    "    * `SelectorBIC`: BIC \n",
    "    * `SelectorDIC`: DIC\n",
    "\n",
    "* Train each word in the training set with a range of values for the number of Hidden States, then Score these alternatives with the Model Selector, and choose the \"best\" according to each strategy. \n",
    "* e.g. the simple case of training with a constant value for `n_components` can be called using the provided `SelectorConstant` subclass as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from my_model_selectors import SelectorConstant\n",
    "\n",
    "# Experiment here with different feature sets defined in Part 1\n",
    "# i.e. Feature Set: features_ground, features_norm, features_polar, \n",
    "#                   features_delta\n",
    "training = asl.build_training(features_ground)\n",
    "\n",
    "# Experiment here with different words contained in `training.words`\n",
    "# (i.e. VEGETABLE, CHOCOLATE, BOOK, etc)\n",
    "word = 'VEGETABLE'\n",
    "model = SelectorConstant(training.get_all_sequences(), \n",
    "                         training.get_all_Xlengths(), \n",
    "                         word, n_constant=3).select()\n",
    "print(\"Number of states trained in model for {} is {}\"\n",
    "      .format(word, model.n_components))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Cross-Validation (CV) folds\n",
    "* Scoring the model simply using Log Likelihood calculated from feature sequences it trained on, we expect more complex models to have higher likelihoods, but doesn't inform us which would have a \"better\" likelihood score on unseen data.  The model will likely overfit as complexity is added.  \n",
    "* Estimate the \"better\" Topology model using only training data by comparing scores using Cross-Validation (CV).  \n",
    "* CV techniques include:\n",
    "    * Break-down the training set into \"folds\", rotate which fold is \"left out\" of training.  The \"left out\" fold is scored. Use this as a proxy method of finding the \"best\"  model to use on \"unseen data\". \n",
    "        * e.g. Given a set of word sequences broken-down into three folds using [scikit-learn Kfold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) class object. Implement `SelectorCV` to use this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Experiment here with different feature sets\n",
    "training = asl.build_training(features_ground) \n",
    "\n",
    "# Experiment here with different words\n",
    "word = 'CHOCOLATE' \n",
    "word_sequences = training.get_word_sequences(word)\n",
    "split_method = KFold()\n",
    "for cv_train_idx, cv_test_idx in split_method.split(word_sequences):\n",
    "    print(\"'TRAIN' fold indices: {} 'TEST' fold indices: {}\".format(cv_train_idx, cv_test_idx))  # view indices of the folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Tip:** To run `hmmlearn` training using X,lengths tuples on new folds, subsets must be combined based on the indices given for the folds.\n",
    "* Helper utility is in `asl_utils` module named `combine_sequences` for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Scoring models with other criterion (i.e. BIC and DIC)\n",
    "\n",
    "* **BIC** when used to scoring model topologies will balance fit and complexity within the training set for each word.\n",
    "    * **BIC equation** has a **penalty term** that penalizes \"complexity\" to avoid \"overfitting\". This avoids using **CV** in the selection process.  Refer to these [slides](http://www2.imm.dtu.dk/courses/02433/doc/ch6_slides.pdf) about this criterion and try to use formula mentioned the SelectorBIC implementation.\n",
    "\n",
    "* **DIC** when used as a scoring model topology has **advantages** over BIC, shown by Alain Biem here: [reference](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.6208&rep=rep1&type=pdf) (and also found [here](https://pdfs.semanticscholar.org/ed3d/7c4a5f607201f3848d4c02dd9ba17c791fc2.pdf)).  \n",
    "    * **DIC** scores the ability of a training set to discriminate one word against competing words. It provides a **penalty** if model likelihoods for non-matching words are too similar to model likelihoods for the correct word in the word set (rather than using a penalty term for complexity like in BIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part2_submission'></a>\n",
    "### Model Selection Implementation Submission\n",
    "\n",
    "Implement the following classes in the `my_model_selectors.py` module:\n",
    "    * `SelectorCV`\n",
    "    * `SelectorBIC`\n",
    "    * `SelectorDIC`\n",
    "\n",
    "Run the selectors on the following 5x five words ('FISH', 'BOOK', 'VEGETABLE', 'FUTURE', 'JOHN'), then answer given questions about your results.\n",
    "\n",
    "**Tip:** The `hmmlearn` library may not be able to train or score all models.\n",
    "    * Implement `try/except` contructs as necessary to eliminate non-viable models from consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words_to_train = ['FISH', 'BOOK', 'VEGETABLE', 'FUTURE', 'JOHN']\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Implement SelectorCV in my_model_selector.py\n",
    "from my_model_selectors import SelectorCV\n",
    "\n",
    "# Experiment here with different feature sets defined in Part 1\n",
    "training = asl.build_training(features_ground)\n",
    "sequences = training.get_all_sequences()\n",
    "Xlengths = training.get_all_Xlengths()\n",
    "for word in words_to_train:\n",
    "    start = timeit.default_timer()\n",
    "    model = SelectorCV(sequences, \n",
    "                       Xlengths, \n",
    "                       word,\n",
    "                       min_n_components=2, \n",
    "                       max_n_components=15, \n",
    "                       random_state = 14).select()\n",
    "    end = timeit.default_timer() - start\n",
    "    if model is not None:\n",
    "        print(\"Training complete for {} with {} states with time {} seconds\".format(word, model.n_components, end))\n",
    "    else:\n",
    "        print(\"Training failed for {}\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DONE: Implement SelectorBIC in module my_model_selectors.py\n",
    "from my_model_selectors import SelectorBIC\n",
    "\n",
    "# Experiment here with different feature sets defined in Part 1\n",
    "training = asl.build_training(features_ground)\n",
    "sequences = training.get_all_sequences()\n",
    "Xlengths = training.get_all_Xlengths()\n",
    "for word in words_to_train:\n",
    "    start = timeit.default_timer()\n",
    "    model = SelectorBIC(sequences, Xlengths, word, \n",
    "                    min_n_components=2, max_n_components=15, random_state = 14).select()\n",
    "    end = timeit.default_timer()-start\n",
    "    if model is not None:\n",
    "        print(\"Training complete for {} with {} states with time {} seconds\".format(word, model.n_components, end))\n",
    "    else:\n",
    "        print(\"Training failed for {}\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Implement SelectorDIC in module my_model_selectors.py\n",
    "from my_model_selectors import SelectorDIC\n",
    "\n",
    "training = asl.build_training(features_ground)  # Experiment here with different feature sets defined in part 1\n",
    "sequences = training.get_all_sequences()\n",
    "Xlengths = training.get_all_Xlengths()\n",
    "for word in words_to_train:\n",
    "    start = timeit.default_timer()\n",
    "    model = SelectorDIC(sequences, Xlengths, word, \n",
    "                    min_n_components=2, max_n_components=15, random_state = 14).select()\n",
    "    end = timeit.default_timer()-start\n",
    "    if model is not None:\n",
    "        print(\"Training complete for {} with {} states with time {} seconds\".format(word, model.n_components, end))\n",
    "    else:\n",
    "        print(\"Training failed for {}\".format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 2:**  Compare and contrast the possible advantages and disadvantages of the various model selectors implemented.\n",
    "\n",
    "**Answer 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part2_test'></a>\n",
    "### Model Selector Unit Testing\n",
    "Run the following unit tests as a sanity check on the implemented model selectors.  The test simply looks for valid interfaces  but is not exhaustive. However, the project should not be submitted if these tests don't pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from asl_test_model_selectors import TestSelectors\n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestSelectors())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part3_tutorial'></a>\n",
    "## PART 3: Recognizer\n",
    "The objective of this section is to \"put it all together\".  Using the four feature sets created and the three model selectors, you will experiment with the models and present your results.  Instead of training only five specific words as in the previous section, train the entire set with a feature set and model selector strategy.  \n",
    "### Recognizer Tutorial\n",
    "##### Train the full training set\n",
    "The following example trains the entire set with the example `features_ground` and `SelectorConstant` features and model selector.  Use this pattern for you experimentation and final submission cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# autoreload for automatically reloading changes made in my_model_selectors and my_recognizer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from my_model_selectors import SelectorConstant\n",
    "\n",
    "def train_all_words(features, model_selector):\n",
    "    training = asl.build_training(features)  # Experiment here with different feature sets defined in part 1\n",
    "    sequences = training.get_all_sequences()\n",
    "    Xlengths = training.get_all_Xlengths()\n",
    "    model_dict = {}\n",
    "    for word in training.words:\n",
    "        model = model_selector(sequences, Xlengths, word, \n",
    "                        n_constant=3).select()\n",
    "        model_dict[word]=model\n",
    "    return model_dict\n",
    "\n",
    "models = train_all_words(features_ground, SelectorConstant)\n",
    "print(\"Number of word models returned = {}\".format(len(models)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Load the test set\n",
    "The `build_test` method in `ASLdb` is similar to the `build_training` method already presented, but there are a few differences:\n",
    "- the object is type `SinglesData` \n",
    "- the internal dictionary keys are the index of the test word rather than the word itself\n",
    "- the getter methods are `get_all_sequences`, `get_all_Xlengths`, `get_item_sequences` and `get_item_Xlengths`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_set = asl.build_test(features_ground)\n",
    "print(\"Number of test set items: {}\".format(test_set.num_items))\n",
    "print(\"Number of test set sentences: {}\".format(len(test_set.sentences_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part3_submission'></a>\n",
    "### Recognizer Implementation Submission\n",
    "For the final project submission, students must implement a recognizer following guidance in the `my_recognizer.py` module.  Experiment with the four feature sets and the three model selection methods (that's 12 possible combinations). You can add and remove cells for experimentation or run the recognizers locally in some other way during your experiments, but retain the results for your discussion.  For submission, you will provide code cells of **only three** interesting combinations for your discussion (see questions below). At least one of these should produce a word error rate of less than 60%, i.e. WER < 0.60 . \n",
    "\n",
    "**Tip:** The hmmlearn library may not be able to train or score all models.  Implement try/except contructs as necessary to eliminate non-viable models from consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO implement the recognize method in my_recognizer\n",
    "from my_recognizer import recognize\n",
    "from asl_utils import show_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO Choose a feature set and model selector\n",
    "features = features_ground # change as needed\n",
    "model_selector = SelectorConstant # change as needed\n",
    "\n",
    "# TODO Recognize the test set and display the result with the show_errors method\n",
    "models = train_all_words(features, model_selector)\n",
    "test_set = asl.build_test(features)\n",
    "probabilities, guesses = recognize(models, test_set)\n",
    "show_errors(guesses, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO Choose a feature set and model selector\n",
    "# TODO Recognize the test set and display the result with the show_errors method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO Choose a feature set and model selector\n",
    "# TODO Recognize the test set and display the result with the show_errors method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 3:**  Summarize the error results from three combinations of features and model selectors.  What was the \"best\" combination and why?  What additional information might we use to improve our WER?  For more insight on improving WER, take a look at the introduction to Part 4.\n",
    "\n",
    "**Answer 3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part3_test'></a>\n",
    "### Recognizer Unit Tests\n",
    "Run the following unit tests as a sanity check on the defined recognizer.  The test simply looks for some valid values but is not exhaustive. However, the project should not be submitted if these tests don't pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from asl_test_recognizer import TestRecognize\n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestRecognize())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part4_info'></a>\n",
    "## PART 4: (OPTIONAL)  Improve the WER with Language Models\n",
    "We've squeezed just about as much as we can out of the model and still only get about 50% of the words right! Surely we can do better than that.  Probability to the rescue again in the form of [statistical language models (SLM)](https://en.wikipedia.org/wiki/Language_model).  The basic idea is that each word has some probability of occurrence within the set, and some probability that it is adjacent to specific other words. We can use that additional information to make better choices.\n",
    "\n",
    "##### Additional reading and resources\n",
    "- [Introduction to N-grams (Stanford Jurafsky slides)](https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf)\n",
    "- [Speech Recognition Techniques for a Sign Language Recognition System, Philippe Dreuw et al](https://www-i6.informatik.rwth-aachen.de/publications/download/154/Dreuw--2007.pdf) see the improved results of applying LM on *this* data!\n",
    "- [SLM data for *this* ASL dataset](ftp://wasserstoff.informatik.rwth-aachen.de/pub/rwth-boston-104/lm/)\n",
    "\n",
    "##### Optional challenge\n",
    "The recognizer you implemented in Part 3 is equivalent to a \"0-gram\" SLM.  Improve the WER with the SLM data provided with the data set in the link above using \"1-gram\", \"2-gram\", and/or \"3-gram\" statistics. The `probabilities` data you've already calculated will be useful and can be turned into a pandas DataFrame if desired (see next cell).  \n",
    "Good luck!  Share your results with the class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a DataFrame of log likelihoods for the test word items\n",
    "df_probs = pd.DataFrame(data=probabilities)\n",
    "df_probs.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "nbpresent": {
   "slides": {
    "0a2d4faf-9fb8-4cee-853b-ed68b90f3f8a": {
     "id": "0a2d4faf-9fb8-4cee-853b-ed68b90f3f8a",
     "prev": null,
     "regions": {
      "3fb9ce83-fbb2-4995-832a-f8f400734ad3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1dbb9346-179b-4835-b430-6369d88f1a1b",
        "part": "whole"
       },
       "id": "3fb9ce83-fbb2-4995-832a-f8f400734ad3"
      }
     }
    },
    "1519a4fa-1588-4644-98de-9c43bf0aceb5": {
     "id": "1519a4fa-1588-4644-98de-9c43bf0aceb5",
     "prev": "8a712017-49b7-449f-8264-43a032ace902",
     "regions": {
      "29546121-ed11-44b7-8144-0c44e874098f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "365590a4-6963-4812-a1cf-688f7b6bb9ff",
        "part": "whole"
       },
       "id": "29546121-ed11-44b7-8144-0c44e874098f"
      }
     }
    },
    "176eaccb-15dd-455d-bf07-504213e7aa01": {
     "id": "176eaccb-15dd-455d-bf07-504213e7aa01",
     "prev": "de6b30f4-2463-4901-92ed-aabad78e5e0f",
     "regions": {
      "1542aa9e-dc55-4b90-adef-bf5181872b42": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5c242050-c1f7-4b3b-8103-2ea9d71a40dc",
        "part": "whole"
       },
       "id": "1542aa9e-dc55-4b90-adef-bf5181872b42"
      }
     }
    },
    "19091b36-b0e7-49b1-b501-ec05937e0da9": {
     "id": "19091b36-b0e7-49b1-b501-ec05937e0da9",
     "prev": "1983c02e-fb99-4c05-a728-e0c0ad7c06d8",
     "regions": {
      "6529a31c-8d45-425c-b1d7-d0ac6fca6a32": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e766909d-9421-4aaf-9fb1-bc90d27e49e3",
        "part": "whole"
       },
       "id": "6529a31c-8d45-425c-b1d7-d0ac6fca6a32"
      }
     }
    },
    "1983c02e-fb99-4c05-a728-e0c0ad7c06d8": {
     "id": "1983c02e-fb99-4c05-a728-e0c0ad7c06d8",
     "prev": "176eaccb-15dd-455d-bf07-504213e7aa01",
     "regions": {
      "1c4e605d-7f22-4f30-b3fb-74b2937e7a4a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4d217204-e5c0-4568-bd30-12c2e41b681d",
        "part": "whole"
       },
       "id": "1c4e605d-7f22-4f30-b3fb-74b2937e7a4a"
      }
     }
    },
    "212b111f-4527-459c-8297-1db5580ee5c9": {
     "id": "212b111f-4527-459c-8297-1db5580ee5c9",
     "prev": "76898529-e49e-4663-8d02-8261dfe1d94b",
     "regions": {
      "2e4bd280-3cd6-47d0-9c81-17737b24053b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "0c316996-9933-4b3d-82ec-259518dc8bc9",
        "part": "whole"
       },
       "id": "2e4bd280-3cd6-47d0-9c81-17737b24053b"
      }
     }
    },
    "23a7337f-a0cf-4ed4-baa9-ec06bfdc0579": {
     "id": "23a7337f-a0cf-4ed4-baa9-ec06bfdc0579",
     "prev": "e76e9a02-54c1-4ec9-80fb-c611ed398122",
     "regions": {
      "b5721d20-d6f8-4ddb-a5aa-eb16f0cc8893": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "313015a2-b5a9-4136-a8ea-5d011e47d840",
        "part": "whole"
       },
       "id": "b5721d20-d6f8-4ddb-a5aa-eb16f0cc8893"
      }
     }
    },
    "732f1952-ee54-46fb-8067-099512824296": {
     "id": "732f1952-ee54-46fb-8067-099512824296",
     "prev": "0a2d4faf-9fb8-4cee-853b-ed68b90f3f8a",
     "regions": {
      "f31d4597-08ad-4c46-ad52-4bd2d775c624": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "aadfec52-27ca-4541-8920-fa9253d51827",
        "part": "whole"
       },
       "id": "f31d4597-08ad-4c46-ad52-4bd2d775c624"
      }
     }
    },
    "76898529-e49e-4663-8d02-8261dfe1d94b": {
     "id": "76898529-e49e-4663-8d02-8261dfe1d94b",
     "prev": "19091b36-b0e7-49b1-b501-ec05937e0da9",
     "regions": {
      "ec1746fc-aec9-4a7c-8225-9e9ac8d45889": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b3e539be-84e2-49ce-a183-31cfc5c7ce7c",
        "part": "whole"
       },
       "id": "ec1746fc-aec9-4a7c-8225-9e9ac8d45889"
      }
     }
    },
    "8a712017-49b7-449f-8264-43a032ace902": {
     "id": "8a712017-49b7-449f-8264-43a032ace902",
     "prev": "bed9e696-630e-4747-be1c-bc3737ba992f",
     "regions": {
      "1faab517-cd16-4c63-bb01-a67246749d7a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3f14ddf0-4145-4687-9c33-712c3c32520f",
        "part": "whole"
       },
       "id": "1faab517-cd16-4c63-bb01-a67246749d7a"
      }
     }
    },
    "90af992d-eb6d-4496-b2d2-6aa9a95b6a61": {
     "id": "90af992d-eb6d-4496-b2d2-6aa9a95b6a61",
     "prev": "732f1952-ee54-46fb-8067-099512824296",
     "regions": {
      "4f448bec-5be9-4553-88ae-e35ed7612f25": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c445fbfb-b8ab-4e9a-8d13-12231a1c588f",
        "part": "whole"
       },
       "id": "4f448bec-5be9-4553-88ae-e35ed7612f25"
      }
     }
    },
    "bed9e696-630e-4747-be1c-bc3737ba992f": {
     "id": "bed9e696-630e-4747-be1c-bc3737ba992f",
     "prev": "23a7337f-a0cf-4ed4-baa9-ec06bfdc0579",
     "regions": {
      "ac1513f0-404f-492b-8b42-0313e9a753b0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "18dd2eee-8b6c-4a5e-9539-132d00a7c7e1",
        "part": "whole"
       },
       "id": "ac1513f0-404f-492b-8b42-0313e9a753b0"
      }
     }
    },
    "de6b30f4-2463-4901-92ed-aabad78e5e0f": {
     "id": "de6b30f4-2463-4901-92ed-aabad78e5e0f",
     "prev": "e36b4639-be8c-46f7-a8c9-bcfb134f9fd0",
     "regions": {
      "55ec36e0-362f-4fd3-8060-7cee056039aa": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c3cf461e-4c9e-4dec-99d2-07bfa79cbe23",
        "part": "whole"
       },
       "id": "55ec36e0-362f-4fd3-8060-7cee056039aa"
      }
     }
    },
    "e36b4639-be8c-46f7-a8c9-bcfb134f9fd0": {
     "id": "e36b4639-be8c-46f7-a8c9-bcfb134f9fd0",
     "prev": "1519a4fa-1588-4644-98de-9c43bf0aceb5",
     "regions": {
      "4c1e9714-9ba0-45fd-8a2f-ef80a5c85c2e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6534d4dc-125f-47e6-a022-cf1e0d277174",
        "part": "whole"
       },
       "id": "4c1e9714-9ba0-45fd-8a2f-ef80a5c85c2e"
      }
     }
    },
    "e76e9a02-54c1-4ec9-80fb-c611ed398122": {
     "id": "e76e9a02-54c1-4ec9-80fb-c611ed398122",
     "prev": "90af992d-eb6d-4496-b2d2-6aa9a95b6a61",
     "regions": {
      "9491b84d-193b-40ff-9321-d21eb1ba88d4": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b64ec10e-fa9d-4f3f-907f-6799611ed6b1",
        "part": "whole"
       },
       "id": "9491b84d-193b-40ff-9321-d21eb1ba88d4"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
